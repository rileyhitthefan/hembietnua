---
title: "Lab F-6: Controlling response variability"
date: 2024-06-28T15:18:06+07:00
draft: false
chapter : false
weight: 6
---

**Temperature** is an inference parameter that influences the variabilty of the responses generated by a foundation model.

In this lab, we will demonstrate how setting the temperature affects the model's response. You can think of temperature as a way to control the creativity & variability of the model. A temperature of 0 means no variability - the current version* of the model will return a consistent response to the same prompt. The higher the temperature, the more variation in responses can be expected. In creative scenarios like content creation, a higher temperature can be helpful. In business process scenarios & code generation, a temperature of zero may be best.

_* If the model gets updated, you will likely see a change in responses from the previous version of the model. It's always safest to assume variability in responses over time, even when the temperature is zero._

## Create the Python script

1. Navigate to the **workshop/labs/temperature** folder, and open the file **temperature.py**

![Temperature](/images/2-Bedrock/F-6/1.png)

2. Add the import statements.

   - These statements allow us to use the LangChain library to call Bedrock, access command line arguments, and read environment variables.

```python
import sys
from langchain_community.llms import Bedrock
```

1. Build the function to call Bedrock with the appropriate inference parameters for the model.

   - Here we are instantiating the LangChain Bedrock client, setting the prompt, and setting the temperature.

```python
def get_text_response(input_content, temperature): #text-to-text client function
    
    model_kwargs = { #AI21
        "maxTokens": 1024, 
        "temperature": temperature, 
        "topP": 0.5, 
        "stopSequences": [], 
        "countPenalty": {"scale": 0 }, 
        "presencePenalty": {"scale": 0 }, 
        "frequencyPenalty": {"scale": 0 } 
    }
    
    llm = Bedrock( #create a Bedrock llm client
        model_id="ai21.j2-ultra-v1",
        model_kwargs = model_kwargs
    )
    
    return llm.invoke(input_content) #return a response to the prompt
```

4. Pass the command line parameters to the `get_text_response` function and display the response.

   - We're passing in the first argument (prompt) and second argument (temperature) from the command line.
   - We will call this function in a loop to see how the responses vary.

```python
for i in range(3):
    response = get_text_response(sys.argv[1], float(sys.argv[2]))
    print(response)
```

5. Save the file.

## Run the script
1. Select the **bash terminal** in AWS Cloud9 and change directory.
```bash
cd ~/environment/workshop/labs/temperature
```

2. Run the script from the terminal, setting the temperature to 0.0.
```bash
python temperature.py "Write a haiku about a long journey:" 0.0
```

The results should be displayed in the terminal:
![Temperature](/images/2-Bedrock/F-6/2.png)

_Note that with a temperature of 0.0, the model generates the same response each time._

3. Run the script from the terminal, this time setting the temperature to 1.0.
```bash
python temperature.py "Write a haiku about a long journey:" 1.0
```

The results should be displayed in the terminal:
![Temperature](/images/2-Bedrock/F-6/3.png)

_With a temperature above zero, the model should generate a variety of interesting responses._

{{% notice tip %}}
**Congratulations!**\
You have successfully demonstrated how temperature can be used to influence responses!
{{% /notice %}}